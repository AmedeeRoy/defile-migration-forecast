{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9K5DwFHRp6h"
   },
   "source": [
    "TODO (for Amédée):\n",
    "\n",
    "- Check normalization in Dataset. Not sure what to do we the conversion to tensor in getitem() in `if self.transform:`\n",
    "> * Normalization changed for environmental covariates (no climatology removed to help the network to learn the annual patterns)\n",
    "> * Not sure to understand the pb is transformation to tensor\n",
    "\n",
    "\n",
    "- Is it a good idea to return out in the forward function in 3D (batch, 1, 24), why not 2D (batch, 24)? It would make the loss computation slightly more convininent to write.\n",
    "> * Good point but I have actually no idea.. We could check by comparing time in loss computation.\n",
    "\n",
    "\n",
    "OTHER MODIFICATIONS\n",
    "> * Year added in dataloader & network inputs\n",
    "> * Change batch size to 256 (usually as big as possible for increased convergence)\n",
    "> * Add learning rate change strategy\n",
    "> * Add gradient loss for helping prediction smoothness\n",
    "> * Add evaluation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcOMrC8cL9qc"
   },
   "source": [
    "# Deep Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1707729145076,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "VnPdd26KMOID",
    "outputId": "eda1d34b-f3a7-41f9-b911-c770f0869da9"
   },
   "outputs": [],
   "source": [
    "# # mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3787,
     "status": "ok",
     "timestamp": 1707729148860,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "12Xd2NhgL9qe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TV0X1OiL9qf"
   },
   "source": [
    "## Define Birddataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2548,
     "status": "ok",
     "timestamp": 1707729151405,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "M20-OGhML9qf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BirdDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, species=\"Buse variable\", years=range(1966, 2023), lag_day=7, transform=False\n",
    "    ):\n",
    "\n",
    "        # data_folder = \"./drive/MyDrive/DefileCast/data\" # for colab\n",
    "        data_folder = \"../data\"  # for local\n",
    "\n",
    "        # WEATHER DATA ----------------------------\n",
    "        # Create data xarray (better to handle multi-indexing)\n",
    "        era5_hourly = pd.read_csv(data_folder + \"/era5_hourly.csv\", parse_dates=[\"datetime\"])\n",
    "        era5_hourly[\"date\"] = pd.to_datetime(era5_hourly[\"datetime\"].dt.date)\n",
    "        era5_hourly[\"time\"] = era5_hourly.datetime.dt.time\n",
    "        era5_hourly = era5_hourly.drop(\"datetime\", axis=1)\n",
    "        era5_hourly = era5_hourly.set_index(\n",
    "            [\"date\", \"time\"]\n",
    "        ).to_xarray()  # date and time as distinct indexes\n",
    "\n",
    "        # Create daily data (with lags)\n",
    "        era5_daily = era5_hourly.mean(dim=\"time\")  # get daily mean\n",
    "        era5_daily = era5_daily.assign_coords(lag=[0])  # add lag as new coordinate\n",
    "        # Make all existing variables depend on the new coordinate\n",
    "        for var in era5_daily.data_vars:\n",
    "            era5_daily[var] = era5_daily[var].expand_dims({\"lag\": era5_daily.lag})\n",
    "\n",
    "        # Shift and merge daily data\n",
    "        era5_daily_lagged = era5_daily.copy()\n",
    "        for lag in range(1, lag_day):\n",
    "            df = era5_daily.shift(date=lag)\n",
    "            df = df.assign_coords(lag=[lag])\n",
    "            era5_daily_lagged = era5_daily_lagged.merge(df.copy())\n",
    "\n",
    "        #  Remove all dates with NaN\n",
    "        # (-> to guarantee that each item has the same size)\n",
    "        # (= equivalent to removing date when no lags are available)\n",
    "        era5_daily_lagged = era5_daily_lagged.dropna(dim=\"date\")\n",
    "\n",
    "        # check that no NaN values are remaining -> ok !\n",
    "        # print('Remaining NaN in ERA5 daily :', era5_daily_lagged.isnull().sum())\n",
    "\n",
    "        # COUNT DATA ----------------------------\n",
    "        # Read data\n",
    "        df = pd.read_csv(\n",
    "            data_folder + \"/all_count_processed.csv\", parse_dates=[\"date\", \"start\", \"end\"]\n",
    "        )\n",
    "        df[\"duration\"] = df[\"end\"] - df[\"start\"]\n",
    "        df[\"doy\"] = df[\"date\"].dt.day_of_year\n",
    "        df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "        # Check that ERA5 values are available for all observations -> ok !\n",
    "        # Otherwise would need to subset the count dataset\n",
    "        # print('Number of dates not in ERA5 daily :', len([d for d in df.date.unique() if d not in era5_daily_lagged.date]))\n",
    "\n",
    "        # Filter data by years\n",
    "        dfy = df[df[\"date\"].dt.year.isin(years)]\n",
    "\n",
    "        # Filter data by species\n",
    "        data_count = dfy[dfy.species == species][[\"date\", \"count\", \"start\", \"end\"]]\n",
    "        dfys = (\n",
    "            dfy[[x for x in list(dfy) if x not in [\"species\", \"count\"]]]\n",
    "            .drop_duplicates()\n",
    "            .merge(data_count, how=\"left\")\n",
    "        )\n",
    "        dfys[\"count\"] = dfys[\"count\"].fillna(0)\n",
    "\n",
    "        # Create mask\n",
    "        # Corresponding to the fraction of each hour of the day during which the count in question has been happening\n",
    "        hours_mat = np.repeat(np.arange(24), len(dfys)).reshape(24, len(dfys))\n",
    "        startHour = dfys[\"start\"].dt.hour.values + dfys[\"start\"].dt.minute.values / 60\n",
    "        endHour = dfys[\"end\"].dt.hour.values + dfys[\"end\"].dt.minute.values / 60\n",
    "        tmp1 = np.maximum(np.minimum(hours_mat - startHour + 1, 1), 0)\n",
    "        tmp2 = np.maximum(np.minimum(endHour - hours_mat, 1), 0)\n",
    "        mask = np.minimum(tmp1, tmp2)\n",
    "\n",
    "        # Check mask is never 0\n",
    "        # mask.sum(axis=0)\n",
    "\n",
    "        # normalizing\n",
    "        if transform:\n",
    "            # era5_daily_lagged = (era5_daily_lagged - era5_daily_lagged.mean(dim = \"date\")) / era5_daily_lagged.std(dim = \"date\")\n",
    "            # era5_hourly = (era5_hourly - era5_hourly.mean(dim = \"date\")) / era5_hourly.std(dim = \"date\")\n",
    "            era5_daily_lagged = (\n",
    "                era5_daily_lagged - era5_daily_lagged.mean()\n",
    "            ) / era5_daily_lagged.std()\n",
    "            era5_hourly = (era5_hourly - era5_hourly.mean()) / era5_hourly.std()\n",
    "\n",
    "            dfys[\"count\"] = np.log10(1 + dfys[\"count\"])\n",
    "            dfys[\"doy\"] = dfys[\"doy\"] / 365\n",
    "            dfys[\"year\"] = (dfys[\"year\"] - 2000) / 100\n",
    "\n",
    "        # Assign to self\n",
    "        self.data = dfys.reset_index(drop=True)\n",
    "        self.era5_daily = era5_daily_lagged\n",
    "        self.era5_hourly = era5_hourly\n",
    "        self.mask = mask\n",
    "        self.lag_day = lag_day\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # index by count/observation\n",
    "        count = self.data[\"count\"][idx]\n",
    "        doy = self.data[\"doy\"][idx]\n",
    "        yr = self.data[\"year\"][idx]\n",
    "        m = self.mask[:, idx]\n",
    "\n",
    "        date = self.data[\"date\"][idx]\n",
    "        era5_h = self.era5_hourly.sel(date=date)\n",
    "        era5_d = self.era5_daily.sel(date=date)\n",
    "\n",
    "        # convert to numpy before transformations\n",
    "        sample = count, yr, doy, era5_h, era5_d, m\n",
    "\n",
    "        # apply transformations\n",
    "        if self.transform:\n",
    "            # to array\n",
    "            sample = (\n",
    "                np.array([count]),\n",
    "                np.array([yr]),\n",
    "                np.array([doy]),\n",
    "                era5_h.to_array().values,\n",
    "                era5_d.to_array().values,\n",
    "                m,\n",
    "            )\n",
    "            # to tensor\n",
    "            sample = tuple([torch.FloatTensor(s) for s in sample])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93J8xo_hL9qg"
   },
   "source": [
    "## Define dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../data/all_count_processed.csv\", parse_dates=[\"date\", \"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1707729151406,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "z9sFwymKRp6m"
   },
   "outputs": [],
   "source": [
    "# split dataset years based on type of data collected\n",
    "yr_grp = [\n",
    "    np.arange(1966, 1992),  # size 26\n",
    "    np.arange(1993, 2013),  # size 20\n",
    "    np.arange(2014, 2021),\n",
    "]  # size 7\n",
    "\n",
    "# Shuffle order\n",
    "np.random.seed(0)\n",
    "[np.random.shuffle(y) for y in yr_grp]\n",
    "\n",
    "# define the cumulative ratio of the training, validation and test dataset.\n",
    "cum_ratios = np.array([0.7, 0.9])  # test size is automatically computed as the fraction left\n",
    "\n",
    "ytraining = []\n",
    "yval = []\n",
    "ytest = []\n",
    "\n",
    "for y in yr_grp:\n",
    "    sz = (len(y) * cum_ratios).astype(int)\n",
    "    y_data = np.split(y, sz)\n",
    "    ytraining.extend(y_data[0])\n",
    "    yval.extend(y_data[1])\n",
    "    ytest.extend(y_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18581,
     "status": "ok",
     "timestamp": 1707729169982,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "EGXfU5kWL9qh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Split dataset into train, val & test\n",
    "train_dataset = BirdDataset(species=\"Buse variable\", years=ytraining, transform=True)\n",
    "val_dataset = BirdDataset(species=\"Buse variable\", years=yval, transform=True)\n",
    "test_dataset = BirdDataset(species=\"Buse variable\", years=ytest, transform=True)\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BirdDataset(species=\"Buse variable\", years=ytest, transform=False)\n",
    "ds = test_dataset.era5_hourly.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"time\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"time\"] = pd.to_timedelta(ds[\"time\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "eval_count_obs = []\n",
    "eval_count_mask = []\n",
    "\n",
    "for count, yr, doy, era5_hourly, era5_daily, mask in iter(test_dataloader):\n",
    "\n",
    "    eval_count_obs.append(count)\n",
    "    eval_count_mask.append(mask)\n",
    "\n",
    "eval_count_obs = torch.cat(eval_count_obs, dim=0)\n",
    "eval_count_mask = torch.cat(eval_count_mask, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVujDKcv_9eH"
   },
   "source": [
    "## Define NN model\n",
    "\n",
    "Ici une architecture simple d'un réseau convolutionnel, hésite pas à jouer avec pour comprendre comment ça marche. La ça transforme juste une série temporelle (à l'heure) de cdtion météo en observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1707729169983,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "Ykl-Lg8vL9qh"
   },
   "outputs": [],
   "source": [
    "# Define model architectire\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Define your model (flexible way)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_features, nb_layer, device):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.nb_features = nb_features\n",
    "        self.nb_layer = nb_layer\n",
    "        self.device = device\n",
    "\n",
    "        nb_features_in_layer = nb_features\n",
    "\n",
    "        # Add layers as per your requirement\n",
    "        layers = []\n",
    "        for n in range(nb_layer):\n",
    "            layers.append(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=nb_features_in_layer,\n",
    "                    out_channels=nb_features_in_layer * 2,\n",
    "                    kernel_size=5,\n",
    "                    stride=1,\n",
    "                    padding=2,\n",
    "                    dilation=1,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm1d(num_features=nb_features_in_layer * 2))\n",
    "            layers.append(nn.ReLU())\n",
    "            nb_features_in_layer *= 2\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Add layers as per your requirement\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=nb_features_in_layer,\n",
    "                out_channels=1,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "                dilation=1,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, yr, doy, era5_hourly):\n",
    "        # Define forward pass\n",
    "\n",
    "        # ---------------------------\n",
    "        # Example of a model that only takes doy and era5_hourly\n",
    "        # and applies distinct layers of 1D convolutions\n",
    "\n",
    "        # doy repeated to go from (batch, 1) to (batch, 1, 24)\n",
    "        doy = doy.repeat(1, 24).unsqueeze(1)\n",
    "        yr = yr.repeat(1, 24).unsqueeze(1)\n",
    "        # concatenate doy with era_hourly as feature (batch, nfeatures + 1, 24)\n",
    "        X = torch.cat([era5_hourly, doy, yr], 1)\n",
    "\n",
    "        out = self.layers(X)\n",
    "        out = self.last_layer(out)\n",
    "        out = (\n",
    "            5 * out\n",
    "        )  # car la dernière couche de sigmoid force 0 < out < 1, making possible to generate count data between 10^0 and 10^5\n",
    "\n",
    "        ## !!!!!!!! Usually not good practice to instantiate value in Tensor !!!!!!!!!\n",
    "        # -> it makes the computation of automatic differentiation impossible\n",
    "        # -> usually better to multiply by some mask\n",
    "        # -> Here in practice it does not change anything yet\n",
    "        # # Force count to be zero between 0-? and ?-24 hr\n",
    "        # out[:,:,:6] = 0\n",
    "        # out[:,:,21:] = 0\n",
    "        # Force count to be zero between 0-? and ?-24 hr\n",
    "        pred_mask = np.array([1 for i in range(24)])\n",
    "        pred_mask[:6] = 0\n",
    "        pred_mask[21:] = 0\n",
    "        pred_mask = torch.FloatTensor(pred_mask).repeat(out.shape[0], 1).unsqueeze(1)\n",
    "        out = out * pred_mask.to(self.device)\n",
    "\n",
    "        return out  # (batch, 1, 24)\n",
    "\n",
    "    def loss(self, y_pred, y, mask):\n",
    "\n",
    "        y_pred_start_to_end = torch.sum(y_pred.squeeze() * mask, dim=1)\n",
    "\n",
    "        # Compute a weight for each hour based on which hour of day it is\n",
    "        # w_hour = ( (torch.arange(24) - 12 )**2+1 )\n",
    "        # np.sum(mask, axis = 1)\n",
    "        sum_mask = torch.Tensor(\n",
    "            [\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0.58,\n",
    "                48,\n",
    "                740,\n",
    "                1777,\n",
    "                3334,\n",
    "                4086,\n",
    "                3878,\n",
    "                3719,\n",
    "                3543,\n",
    "                3363,\n",
    "                2995,\n",
    "                2378,\n",
    "                1440,\n",
    "                585,\n",
    "                160,\n",
    "                17,\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "            ]\n",
    "        ).to(self.device)\n",
    "        w_hour = 1 / (1 + sum_mask)\n",
    "        w_hour[6:21] = w_hour[6:21] / sum(w_hour[6:21])\n",
    "\n",
    "        w_count = torch.sum(w_hour * mask, dim=1)\n",
    "\n",
    "        loss_rmse = torch.mean((y_pred_start_to_end - y.squeeze()) ** 2 * w_count)\n",
    "        loss_grad = torch.mean(torch.diff(count_pred, 1) ** 2)\n",
    "\n",
    "        return loss_rmse + loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1790,
     "status": "ok",
     "timestamp": 1707729171766,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "G3Jb6eAqkqwx",
    "outputId": "8bd53bad-cffd-46b9-a528-22fb09e63eca"
   },
   "outputs": [],
   "source": [
    "model = Net(nb_features=7, nb_layer=4, device=\"cpu\")\n",
    "\n",
    "count, yr, doy, era5_hourly, era5_daily, mask = next(iter(train_dataloader))\n",
    "count_pred = model(yr, doy, era5_hourly)\n",
    "print(count_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z6zGxCpATl2"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1707729172117,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "1PvCjXTc753o"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# if GPU available (cf. GG Colab > Exécution > Modifier le type d'exécution)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Net(nb_features=7, nb_layer=4, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)  # or any other optimizer\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 281107,
     "status": "ok",
     "timestamp": 1707729453217,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "jVSM_2BZL9qi",
    "outputId": "55922443-24fa-4481-854a-d080b3c1494b"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "counter = 0\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # TRAINING\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    for count, yr, doy, era5_hourly, era5_daily, mask in tqdm(iter(train_dataloader)):\n",
    "\n",
    "        # push to GPU\n",
    "        count, yr, doy, era5_hourly, era5_daily, mask = (\n",
    "            count.to(device),\n",
    "            yr.to(device),\n",
    "            doy.to(device),\n",
    "            era5_hourly.to(device),\n",
    "            era5_daily.to(device),\n",
    "            mask.to(device),\n",
    "        )\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        count_pred = model(yr, doy, era5_hourly)\n",
    "        loss = model.loss(count_pred, count, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # Update the plot\n",
    "        plt.semilogy(train_loss, c=\"tab:blue\")\n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "    scheduler.step()\n",
    "\n",
    "    # VALIDATION\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    for count, yr, doy, era5_hourly, era5_daily, mask in tqdm(iter(val_dataloader)):\n",
    "        # push to GPU\n",
    "        count, yr, doy, era5_hourly, era5_daily, mask = (\n",
    "            count.to(device),\n",
    "            yr.to(device),\n",
    "            doy.to(device),\n",
    "            era5_hourly.to(device),\n",
    "            era5_daily.to(device),\n",
    "            mask.to(device),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            count_pred = model(yr, doy, era5_hourly)\n",
    "            loss = model.loss(count_pred, count, mask)\n",
    "\n",
    "        # print statistics\n",
    "        val_loss.append(loss.item())\n",
    "    val_loss = np.mean(val_loss)\n",
    "\n",
    "    # Early stopping criterion\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\"state_dict\": model.state_dict()}, \"best_model.pth\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping: No improvement for {} epochs.\".format(patience))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1707729453218,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "X1teaMmSRp6r",
    "outputId": "636a57c3-85ab-4a19-e594-6ce397d3d7e4"
   },
   "outputs": [],
   "source": [
    "# Load the best model for testing\n",
    "best_model_checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(best_model_checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 1934,
     "status": "ok",
     "timestamp": 1707729511380,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "FMjGJ6W3F202",
    "outputId": "ee16a9cd-d710-48d1-ebc1-def66ae65019"
   },
   "outputs": [],
   "source": [
    "# Example of prediction on train dataset\n",
    "count, yr, doy, era5_hourly, era5_daily, mask = next(iter(train_dataloader))\n",
    "# push to GPU\n",
    "count, yr, doy, era5_hourly, era5_daily, mask = (\n",
    "    count.to(device),\n",
    "    yr.to(device),\n",
    "    doy.to(device),\n",
    "    era5_hourly.to(device),\n",
    "    era5_daily.to(device),\n",
    "    mask.to(device),\n",
    ")\n",
    "\n",
    "count_pred = model(yr, doy, era5_hourly)\n",
    "\n",
    "y_pred = count_pred.detach().cpu().numpy()\n",
    "m = mask.detach().cpu().numpy()\n",
    "count = count.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 4), tight_layout=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(6):\n",
    "    ax[i].plot(np.arange(0, 24), y_pred[i, 0, :])\n",
    "    ax[i].plot(np.arange(0, 24), m[i, :])\n",
    "    ax[i].plot(np.arange(0, 24), count[i].repeat(24))\n",
    "    ax[i].set_xlabel(\"hours\")\n",
    "    ax[i].set_ylabel(\"Bird counts (log10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3eTMSlDfdiv"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2436,
     "status": "ok",
     "timestamp": 1707729608035,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "mTgWbvL9L9qi",
    "outputId": "1955b7c4-125c-4c4a-b80a-86f880d8f40d"
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "model.eval()\n",
    "test_count_true = []\n",
    "test_count_pred = []\n",
    "test_mask = []\n",
    "\n",
    "for count, yr, doy, era5_hourly, era5_daily, mask in tqdm(iter(test_dataloader)):\n",
    "    # push to GPU\n",
    "    count, yr, doy, era5_hourly, era5_daily, mask = (\n",
    "        count.to(device),\n",
    "        yr.to(device),\n",
    "        doy.to(device),\n",
    "        era5_hourly.to(device),\n",
    "        era5_daily.to(device),\n",
    "        mask.to(device),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        count_pred = model(yr, doy, era5_hourly)\n",
    "\n",
    "        test_count_true.append(count)\n",
    "        test_count_pred.append(count_pred)\n",
    "        test_mask.append(mask)\n",
    "\n",
    "test_count_true = torch.cat(test_count_true, 0)\n",
    "test_count_pred = torch.cat(test_count_pred, 0)\n",
    "test_mask = torch.cat(test_mask, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 2631,
     "status": "ok",
     "timestamp": 1707729648822,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "Lmv8WmFzfic2",
    "outputId": "1d354ef5-c90e-409e-cfc4-e701f3d04754"
   },
   "outputs": [],
   "source": [
    "test_count_true = test_count_true.detach().cpu().numpy()\n",
    "test_count_pred = test_count_pred.detach().cpu().numpy()\n",
    "test_mask = test_mask.detach().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 4), tight_layout=True, sharey=True, sharex=True)\n",
    "ax = ax.flatten()\n",
    "k = 0\n",
    "for i in range(6):\n",
    "    ax[i].plot(np.arange(0, 24), test_count_pred[k + i, 0, :])\n",
    "    ax[i].plot(np.arange(0, 24), test_mask[k + i, :])\n",
    "    ax[i].plot(np.arange(0, 24), test_count_true[k + i].repeat(24))\n",
    "    ax[i].set_xlabel(\"hours\")\n",
    "    ax[i].set_ylabel(\"Bird counts (log10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1707729695778,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "xC1-y0Yoftz6",
    "outputId": "c8aa32dd-67c3-40b4-8d5b-bdeffabc1a91"
   },
   "outputs": [],
   "source": [
    "plt.scatter(test_count_true.squeeze(), np.sum(test_count_pred.squeeze() * test_mask, 1))\n",
    "plt.plot([0, 3], [0, 3], c=\"tab:grey\", linestyle=\":\")\n",
    "plt.xlabel(\"Observed counts\")\n",
    "plt.ylabel(\"Predicted masked counts\")\n",
    "# plt.xlim(0,3)\n",
    "# plt.xlim(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8236,
     "status": "ok",
     "timestamp": 1707729726949,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "grjkXh8Cfu1t"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "test_dataset_raw = BirdDataset(species=\"Buse variable\", years=ytest, transform=False)\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(test_dataset_raw)):\n",
    "    count, year, doy, era5_hourly, era5_daily, mask = test_dataset_raw[i]\n",
    "    pred = era5_hourly.copy()\n",
    "    pred = pred.assign(count_pred=(\"time\", test_count_pred[i, 0, :]))\n",
    "    predictions.append(pred)\n",
    "\n",
    "predictions = xr.concat(predictions, dim=\"date\")\n",
    "predictions = predictions.groupby(\"date\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1707729755149,
     "user": {
      "displayName": "Amédée Roy",
      "userId": "04831915678617948115"
     },
     "user_tz": -60
    },
    "id": "BsKiGYhqfwKv",
    "outputId": "d669d57b-ed0a-4977-857b-027274d2c5b3"
   },
   "outputs": [],
   "source": [
    "## PREDICTED COUNTS\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "print(np.unique(predictions.date.dt.year))\n",
    "predictions.sel(date=predictions.date.dt.year == 2014).sum(dim=\"time\").count_pred.plot(ax=ax[0])\n",
    "ax[0].set_title(\"Sum of predicted counts by date\")\n",
    "\n",
    "ax[1].plot(predictions.mean(dim=\"date\").count_pred)\n",
    "ax[1].set_title(\"Mean of predicted counts by time\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
