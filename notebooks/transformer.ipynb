{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(os.getcwd(), indicator=\".project-root\", pythonpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.defile_datamodule import DefileDataModule\n",
    "\n",
    "dataloader = DefileDataModule(\n",
    "    data_dir=r\"C:\\Users\\amedee.roy\\OneDrive - FRANCE ENERGIES MARINES\\SCRIPTS\\defile-migration-forecast\\data\",\n",
    "    batch_size=256,\n",
    "    species=\"Buse variable\",\n",
    "    lag_day=7,\n",
    "    seed=0,\n",
    "    train_val_test_cum_ratio=[0.7, 0.9],\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "dataloader.setup()\n",
    "train_dataloader = dataloader.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, yr, doy, era5_hourly, era5_daily, mask = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# from https://medium.com/@mkaanaslan99/time-series-forecasting-with-a-basic-transformer-model-in-pytorch-650f116a1018\n",
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, drop_prob):\n",
    "        super(transformer_block, self).__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.ln1 = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.ln2 = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attention(x, x, x, need_weights=False)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "        fc_out = self.fc(x)\n",
    "        x = x + self.dropout(fc_out)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncodingLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def get_angles(self, positions, indexes):\n",
    "        dim_tensor = torch.FloatTensor([[self.dim]]).to(positions.device)\n",
    "        angle_rates = torch.pow(10000, (2 * (indexes // 2)) / dim_tensor)\n",
    "        return positions / angle_rates\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        \"\"\"\n",
    "        :param Tensor[batch_size, seq_len] input_sequences\n",
    "        :return Tensor[batch_size, seq_len, dim] position_encoding\n",
    "        \"\"\"\n",
    "        positions = (\n",
    "            torch.arange(input_sequences.size(1)).unsqueeze(1).to(input_sequences.device)\n",
    "        )  # [seq_len, 1]\n",
    "        indexes = torch.arange(self.dim).unsqueeze(0).to(input_sequences.device)  # [1, dim]\n",
    "        angles = self.get_angles(positions, indexes)  # [seq_len, dim]\n",
    "        angles[:, 0::2] = torch.sin(angles[:, 0::2])  # apply sin to even indices in the tensor; 2i\n",
    "        angles[:, 1::2] = torch.cos(angles[:, 1::2])  # apply cos to odd indices in the tensor; 2i\n",
    "        position_encoding = angles.unsqueeze(0).repeat(\n",
    "            input_sequences.size(0), 1, 1\n",
    "        )  # [batch_size, seq_len, dim]\n",
    "        return position_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_ = doy.repeat(1, 24).unsqueeze(1)\n",
    "yr_ = yr.repeat(1, 24).unsqueeze(1)\n",
    "x_h = torch.cat([era5_hourly, doy_, yr_], 1)\n",
    "\n",
    "print(x_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_input_features,\n",
    "        embed_size_hourly,\n",
    "        num_heads_hourly,\n",
    "        num_blocks_hourly,\n",
    "        embed_size_daily,\n",
    "        num_heads_daily,\n",
    "        num_blocks_daily,\n",
    "        drop_prob,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncodingLayer(dim=nb_input_features)\n",
    "\n",
    "        # Hourly Network --------------------------\n",
    "        self.cnn_embedding_hourly = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                nb_input_features,\n",
    "                embed_size_hourly,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "                dilation=1,\n",
    "            ),\n",
    "            nn.BatchNorm1d(num_features=embed_size_hourly),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.blocks_hourly = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block(embed_size_hourly, num_heads_hourly, drop_prob)\n",
    "                for n in range(num_blocks_hourly)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.cnn_output_hourly = nn.Sequential(\n",
    "            nn.Conv1d(embed_size_hourly, 1, kernel_size=5, stride=1, padding=2, dilation=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Daily Network --------------------------\n",
    "        self.cnn_embedding_daily = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                nb_input_features, embed_size_daily, kernel_size=5, stride=1, padding=2, dilation=1\n",
    "            ),\n",
    "            nn.BatchNorm1d(num_features=embed_size_daily),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.blocks_daily = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block(embed_size_daily, num_heads_daily, drop_prob)\n",
    "                for n in range(num_blocks_daily)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last_layer_daily = nn.Sequential(\n",
    "            nn.Linear(embed_size_daily, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, yr, doy, era5_hourly, era5_daily):\n",
    "\n",
    "        # Hourly Transformer\n",
    "        doy_ = doy.repeat(1, 24).unsqueeze(1)\n",
    "        yr_ = yr.repeat(1, 24).unsqueeze(1)\n",
    "        x_h = torch.cat([era5_hourly, doy_, yr_], 1)\n",
    "        x_h = x_h + self.positional_encoding(x_h.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        out_h = self.cnn_embedding_hourly(x_h)\n",
    "        out_h = out_h.transpose(1, 2)\n",
    "        for block in self.blocks_hourly:\n",
    "            out_h = block(out_h)\n",
    "        out_h = out_h.transpose(1, 2)\n",
    "        out_h = self.cnn_output_hourly(out_h)\n",
    "\n",
    "        # Daily Transformer\n",
    "        doy_ = doy.repeat(1, 7).unsqueeze(1)\n",
    "        yr_ = yr.repeat(1, 7).unsqueeze(1)\n",
    "        x_d = torch.cat([era5_daily, doy_, yr_], 1)\n",
    "        x_d = x_d + self.positional_encoding(x_d.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        out_d = self.cnn_embedding_daily(x_d)\n",
    "        out_d = out_d.transpose(1, 2)\n",
    "        for block in self.blocks_daily:\n",
    "            out_d = block(out_d)\n",
    "        out_d = torch.mean(out_d, dim=1)\n",
    "        out_d = self.last_layer_daily(out_d).unsqueeze(1)\n",
    "\n",
    "        out = 5 * out_h * out_d\n",
    "\n",
    "        # Force count to be zero between 0-? and ?-24 hr\n",
    "        pred_mask = np.array([1 for i in range(24)])\n",
    "        pred_mask[:6] = 0\n",
    "        pred_mask[21:] = 0\n",
    "        pred_mask = torch.FloatTensor(pred_mask).repeat(out.shape[0], 1).unsqueeze(1)\n",
    "        out = out * pred_mask\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    nb_input_features=7,\n",
    "    embed_size_hourly=64,\n",
    "    num_heads_hourly=8,\n",
    "    num_blocks_hourly=4,\n",
    "    embed_size_daily=32,\n",
    "    num_heads_daily=4,\n",
    "    num_blocks_daily=2,\n",
    "    drop_prob=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(yr, doy, era5_hourly, era5_daily).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defile-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
